{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.adk.agents import Agent\n",
    "from  main import retrieve_content\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "from google.adk.agents import LlmAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c51f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" \n",
    "initial_state = {\"tool_result_list\": []}\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    state = initial_state\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea77fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions = \"\"\"\n",
    "# You are a specialized assistant for processing documents. Your ONLY task is to understand a user's request, extract the necessary information, and then call the `retrieve_content` tool.\n",
    "\n",
    "# Follow these rules strictly:\n",
    "# 1.  You will be given a GCS path and a user prompt.\n",
    "# 2.  From the user's prompt, you MUST extract  a 'heading_number' (as a string) or a 'heading_title' (as a string).\n",
    "# 3.  If both parameters are found, pass both of them, if any one is found then pass the other as None when you call the tool.The gcs path needs to passed in all cases strictly.\n",
    "# 4.  **Error Condition**: If you cannot find ANY explicit heading number or heading title in the user's prompt, DO NOT call the tool. Instead, respond directly to the user with the exact message: \"Error: Your request must include a specific heading number or title.\"\n",
    "# 5. The tool will output a list containing two values in string format, you have to output in the given structure:\n",
    "# [\"Location_1\" : list[0],[\"Location_1\" : list[1]], here list is the output of the tool \"retreival_agent\"\n",
    "# \"\"\"\n",
    "\n",
    "# # 2. Create the Agent instance\n",
    "# agent = LlmAgent(\n",
    "#     name = \"retreival_agent\",\n",
    "#     model=\"gemini-2.5-pro\",\n",
    "#     tools=[retrieve_content],\n",
    "#     instruction=instructions,\n",
    "#     output_key = \"locations\" \n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0ca12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from google.adk.tools import ToolContext\n",
    "\n",
    "async def print_locations(tool_context: ToolContext) -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Print the stored locations from state.\n",
    "    Expects that state[\"locations\"] is a list of GCS paths or empty strings.\n",
    "    \"\"\"\n",
    "    print(f\"  [Tool Call] print_locations triggered by {tool_context.agent_name}\")\n",
    "\n",
    "    # Access the agent's state\n",
    "    state_dict = tool_context.state.to_dict()\n",
    "    locations = state_dict.get(\"locations\", [])\n",
    "\n",
    "    if not isinstance(locations, list):\n",
    "        return {\"error\": \"locations is not a list in state.\"}\n",
    "    if len(locations) == 0:\n",
    "        return {\"error\": \"No locations found.\"}\n",
    "\n",
    "    # Filter out empty strings\n",
    "    non_empty = [loc for loc in locations if loc]\n",
    "\n",
    "    if len(non_empty) < 2:\n",
    "        return {\"error\": \"Not enough valid locations to print.\"}\n",
    "\n",
    "    formatted = f'[\"Location_1\" : {non_empty[0]}, \"Location_2\" : {non_empty[1]}]'\n",
    "    print(f\"Formatted Output: {formatted}\")\n",
    "\n",
    "    return {\"formatted_locations\": formatted}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c137369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a specialized assistant for processing documents. Your ONLY task is to understand a user's request, extract the necessary information, and then call the `retrieve_content` tool.\n",
    "\n",
    "Rules:\n",
    "1. You will be given a GCS path and a user prompt.\n",
    "2. From the user's prompt, you MUST extract a 'heading_number' (string) or a 'heading_title' (string).\n",
    "3. If both are found, pass both; if only one is found, set the other to None. Always pass the GCS path.\n",
    "4. Error Condition: If no heading number or title is found, respond with: \"Error: Your request must include a specific heading number or title.\"\n",
    "5. The `retrieve_content` tool will return a list of two strings (locations). Store this in state as `locations`.\n",
    "6. After calling `retrieve_content`, immediately call `print_locations` to display the output in this format:\n",
    "   [\"Location_1\" : list[0], \"Location_2\" : list[1]]\n",
    "\"\"\"\n",
    "\n",
    "agent = LlmAgent(\n",
    "    name=\"retrieval_agent\",\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    tools=[retrieve_content, print_locations],\n",
    "    instruction=instructions,\n",
    "    output_key=\"locations\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db5ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runner = Runner(\n",
    "    agent=agent, \n",
    "    app_name=APP_NAME,  \n",
    "    session_service=session_service \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee1fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9addf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: GCS File Location : gs://genai_ex_documents/main_docs/g20_pub.pdf , User Prompt:Give me the test cases corresponding to section 5 and 'Progress Control'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g20_pub\n",
      "Downloading gs://genai_ex_documents/main_docs/g20_pub.pdf into memory...\n",
      "Extracting pages: [1, 2, 3]\n",
      "File already exists. Saving instead as: gs://genai_ex_documents/content_pages/g20_pub_toc_copy11.pdf\n",
      "Uploading extracted PDF to gs://genai_ex_documents/content_pages/g20_pub_toc_copy11.pdf\n",
      "Calling API to process: gs://genai_ex_documents/content_pages/g20_pub_toc_copy11.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adini\\anaconda3\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Starting content population...\n",
      "Detected headers/footers: {'APPLICATION SOFTWARE TESTING', 'GUIDELINES FOR', 'Nil'}\n",
      "\n",
      "...Population complete!\n",
      "\n",
      "--- Final Populated JSON ---\n",
      "{\n",
      "  \"1\": {\n",
      "    \"title\": \"PURPOSE\",\n",
      "    \"content\": \"The major purpose of this document is to provide a set of application software testing \\nguidelines, to ensure that computer systems are properly tested, so as to pursue reliable \\nand quality computer systems. \\n \\n \\n \\n\\n \\nSCOPE \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "    \"subsections\": {}\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"title\": \"SCOPE\",\n",
      "    \"content\": \"This document is to give a set of guidelines for reference by application project teams on \\nthe planning and carrying out of testing activities for application software.  It is \\nimportant that users of this document (i.e. application project teams) should not \\ntreat these guidelines as mandatory standards, but as a reference model; and should \\nadopt the guidelines according to individual project\\u2019s actual situation. \\n \\nThis document is suitable for development projects following the SDLC which is defined \\nin the Information Systems Procedures Manual.  As for the maintenance projects, these \\nguidelines may need to be adjusted according to projects\\u2019 actual situation.  Such \\nadjustments will be the responsibility of individual project team. \\n \\nIt is intended that the guidelines would be applicable to both in-house-developed and \\noutsourced software development projects. \\n \\n \\n\\n \\nREFERENCES \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "    \"subsections\": {}\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"title\": \"REFERENCES\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"3.1\": {\n",
      "        \"title\": \"STANDARDS\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"3.2\": {\n",
      "        \"title\": \"OTHER REFERENCES\",\n",
      "        \"content\": \"(a) The Art of Software Testing, Third Edition by Glenford J. Myers, Corey Sandler and \\nTom Badgett \\n(b) A Structured Approach to Systems Testing by William E. Perry \\n(c) IEEE Standard for Software and System Test Documentation by IEEE \\n(d) NCC IT Starts Developers\\u2019 Guide by National Computing Centre \\n(e) J Scheibmeir, J Herschmann (2018), Adopt a \\u201cShift Left\\u201d approach to testing to \\naccelerate and improve application development, Gartner Inc. \\n(f) T Murphy  (2011), Agile and the rest of the organisation: Requirement definition and \\nmanagement, Gartner Inc. \\n(g) M West (2017), Survey analysis: Agile now at the tipping point \\u2013 Here\\u2019s how to \\nsucceed, Gartner Inc. \\n(h) N Wilson  (2013), Managing the Agile project, Gartner Inc. \\n(i) M Hotle, N Wilson (2017), The end of the waterfall as we know it, Gartner Inc. \\n(j) M West (2017), Scrum is not enough: Essential practices for Agile success, Gartner \\nInc. \\n(k) A Rodger (2014), The impact of Agile on IT Governance, Ovum Ltd. \\n(l) C Singh (2013), Agile Requirements Management, Ovum Ltd. \\n(m) C Singh (2013), Continuous delivery for the Agile organisation, Ovum Ltd. \\n(n) S Archer, C Kaufman (2013), Accelerating outcomes with a hybrid approach within a \\nwaterfall environment, Project Management Institute. \\n\\n \\nDEFINITIONS AND \\n \\nCONVENTIONS \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "        \"subsections\": {}\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"4\": {\n",
      "    \"title\": \"DEFINITIONS AND CONVENTIONS\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"4.1\": {\n",
      "        \"title\": \"DEFINITIONS\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"4.2\": {\n",
      "        \"title\": \"CONVENTIONS\",\n",
      "        \"content\": \"OVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "        \"subsections\": {}\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"5\": {\n",
      "    \"title\": \"OVERVIEW\",\n",
      "    \"content\": \"This document, in essence, suggests a reference model for the planning and conduct of \\nApplication Software Testing.  The following serves as an overview of the model:\",\n",
      "    \"subsections\": {\n",
      "      \"5.1\": {\n",
      "        \"title\": \"PROJECT ORGANISATION\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"5.1.1\": {\n",
      "            \"title\": \"Project Organisation Structure\",\n",
      "            \"content\": \"A project organisation is set up to champion, manage and execute an IT project.  Members \\nof the project organisation include the Project Owner, Project Steering Committee (PSC), \\nProject Assurance Team (PAT), Internal Project Manager (IPM) and Business Analyst \\n(BA). \\n \\nThe PSC champions the project and is the ultimate decision-maker for the project.  It \\nprovides steer and support for the IPM and endorses acceptance of project deliverables. \\nThe IPM manages the project and monitors the project implementation on a day-to-day \\nbasis for the Project Owner/the PSC. \\n \\nThe PAT is responsible for overseeing project progress and managing quality assurance \\nactivities, which include: \\n(a) recommending the test plans, test specifications and test summary report for \\nendorsement by the PSC; and \\n(b) co-ordinating, monitoring and resolving priority conflicts on the testing activities to \\nensure smooth running of testing activities.  \\n \\nPlease refer to the Practice Guide to Project Management for IT Projects under an \\nOutsourced Environment (PGPM) for more details of the project organisation.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"5.1.2\": {\n",
      "            \"title\": \"Test Group\",\n",
      "            \"content\": \"Testing is the process of executing a program with the intent of finding errors.  Since it is \\nsuch a \\u201cdestructive\\u201d process, it may be more effective and successful if the testing is \\nperformed by an independent third party other than the original system analysts / \\nprogrammers. \\n \\nAs far as possible, testing should be performed by a group of people different from those \\nperforming design and coding of the same system.  That group of people is called the Test \\nGroup. \\n \\nA Test Group can be set up to carry out the testing activities especially for large-scale \\nprojects or projects involving a large number of users.  The emphasis here is on the \\nindependent role of the Test Group, which does not necessarily mean dedicated resources.  \\nThe necessity of an independent Test Group should be determined at the project \\ninitialisation stage through an assessment based on project complexity, criticality, \\nimplementation schedule and other risks.  The type(s) of testing to be performed \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-2 \\n  \\nindependently and the high level estimation of additional resources, if required, should be \\ndetermined and planned for respectively as early as possible. \\n \\nThe following figure shows an example of project organisation with the formation of a \\nTest Group and an optional Independent Testing Contractor providing independent testing \\nservice.   It is noted that the independent testing may be conducted by a Test Group of in-\\nhouse staff members as well as by external contractor.  \\n \\n \\n \\nFigure 1 - Example of Project Organisation with Test Group and Independent \\nTesting Contractor\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"5.2\": {\n",
      "        \"title\": \"TESTING ACTIVITIES\",\n",
      "        \"content\": \"With reference to the Agile Software Development Method, test preparation should be \\nstarted as early as possible and constant communication should be maintained with \\nrelevant stakeholders to facilitate collaboration and transparency.  The following activities \\nare suggested: \\n \\n(i) \\nThe IPM should develop a high level test plan covering all major test types during \\nproject initiation with the objectives, scope of testing and the composition of Test \\nGroup including contractors, business users and internal IT staff with defined \\nroles and responsibilities. \\n \\n(ii) \\nContractor project manager (or IPM for in-house developed project) to enrich the \\ntest plans by engaging his/her staff to draft test cases; internal IT staff to check all \\nmajor test plans; and business users to provide different test cases to address \\ndifferent scenarios; and \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-3 \\n  \\n \\n(iii) Contractor project manager (or IPM for in-house developed project) to maintain \\nongoing communication and collaboration among stakeholders by distributing all \\nmajor test plans and feedbacks to stakeholders regularly to keep them informed \\nthe project progress throughout the whole system development stage. \\n \\nA computer system is subject to testing from the following five different perspectives: \\n \\n(i) \\nTo validate individual program modules against program specifications (Unit \\nTesting); \\n \\n(ii) \\nTo validate linkages or interfaces between program modules against design \\nspecifications (Link/Integration Testing); \\n \\n(iii) To validate integrated software against functional specifications (Function \\nTesting); \\n \\n(iv) To validate the integrated software against specifications on operating \\nenvironment (System Testing); and, \\n \\n(v) \\nTo validate the integrated software against end-user needs and business \\nrequirements (Acceptance Testing). \\n \\n(Refer to Section 7) \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-4\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"5.3\": {\n",
      "        \"title\": \"TEST DOCUMENTATION\",\n",
      "        \"content\": \"To document testing activities through the use of \\n \\n(i) \\nTest Plan \\n(ii) \\nTest Specification \\n(iii) \\nTest Incident Report \\n(iv) \\nTest Progress Report \\n(v) \\nTest Summary Report \\n \\n(Refer to Section 8)\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"5.4\": {\n",
      "        \"title\": \"TEST PLANNING AND CONTROL\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"5.4.1\": {\n",
      "            \"title\": \"Progress Control\",\n",
      "            \"content\": \"Monitor the day-to-day progress of the testing activities through the use of Test Progress \\nReports. \\n \\n(Refer to Section 8.5)\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"5.4.2\": {\n",
      "            \"title\": \"Quality Control / Assurance\",\n",
      "            \"content\": \"Testing documentation to be compiled by Test Group or Independent Testing Contractor \\nif outsourced, cross-checked by quality assurance staff2, and reviewed by the PAT.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"5.4.3\": {\n",
      "            \"title\": \"Resource Estimation\",\n",
      "            \"content\": \"Project teams may update the testing metrics information to a centralised database for \\nfuture test planning references. \\n \\n                                                 \\n2 Quality assurance staff should be the IPM or other delegated staff.  However, those who are the members of the Test \\nGroup should not take up the quality assurance role for the project if the tests are conducted by them but not by \\nIndependent Testing Contractor. \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"6\": {\n",
      "    \"title\": \"GENERAL CONCEPTS OF TESTING\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"6.1\": {\n",
      "        \"title\": \"TESTING OBJECTIVES\",\n",
      "        \"content\": \"Testing is the process of executing program(s) with the intent of finding errors, rather than \\n(a misconception) showing the correct functioning of the program(s).  The difference \\nactually lies on the different psychological effect caused by the different objectives:  If \\nour goal is to demonstrate that a program has no errors, then we will tend to select tests \\nthat have a low probability of causing the program to fail.  On the other hand, if our goal \\nis to demonstrate that a program has errors, our test data will have a higher probability of \\nfinding errors. \\n \\nSpecifically, testing should bear the following objectives: \\n \\n(a) \\nto reveal design errors; \\n(b) \\nto reveal logic errors; \\n(c) \\nto reveal performance bottleneck; \\n(d) \\nto reveal security loophole; and \\n(e) \\nto reveal operational deficiencies. \\n \\nAll these objectives and the corresponding actions contribute to improve the quality and \\nreliability of the application software.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"6.2\": {\n",
      "        \"title\": \"TESTING APPROACH\",\n",
      "        \"content\": \"There are two approaches to software testing, namely White-Box Testing and Black-Box \\nTesting. \\n \\nWhite-Box Testing, also known as Code Testing, focuses on the independent logical \\ninternals of the software to assure that all code statements and logical paths have been \\ntested. \\n \\nBlack-Box Testing, also known as Specification Testing, focuses on the functional \\nexternals to assure that defined input will produce actual results that agree with required \\nresults documented in the specifications. \\n \\nBoth approaches should be used, according to the level of testing. \\n \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    6-2\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"6.3\": {\n",
      "        \"title\": \"LEVELS OF TESTING\",\n",
      "        \"content\": \"There are 5 levels of Testing, each of which carries a specific functional purpose, to be \\ncarried out in chronological order. \\n \\n \\n \\nTesting \\nApproach Applied \\n(a) Unit Testing \\n \\n \\n- Testing of the program modules in isolation with \\nthe objective to find discrepancy between the \\nprograms and the program specifications \\nWhite Box Test \\n \\n \\n \\n(b) Link/Integration Testing \\n \\n \\n- Testing of the linkages or interfaces between \\ntested program modules with the objective to find \\ndiscrepancy between the programs and system \\nspecifications \\nWhite Box Test \\n \\n \\n \\n(c) Function Testing \\n \\n \\n- Testing of the integrated software on a function \\nby function basis with the objective to find \\ndiscrepancy between the programs and the \\nfunction specifications \\nBlack Box Test \\n \\n \\n \\n \\n(d) System Testing \\n \\n \\n- Testing of the integrated software with the \\nobjective to find discrepancy between the \\nprograms and the original objectives with regard \\nto the operating environment of the system (e.g. \\nRecovery, Security, Performance, Storage, etc.) \\nBlack Box Test \\n \\n \\n \\n \\n(e) Acceptance Testing \\n \\n \\n- Testing of the integrated software by the end-\\nusers (or their proxy) with the objective to find \\ndiscrepancy between the programs and the end-\\nuser needs and business requirements \\nBlack Box Test \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    6-3\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"6.4\": {\n",
      "        \"title\": \"GENERAL TESTING PRINCIPLES\",\n",
      "        \"content\": \"The following points should be noted when conducting testing: \\n \\n(a) \\nAs far as possible, testing should be performed by a group of people (referred to in \\nthis document as Test Group) different from those performing design and coding of \\nthe same system.  For large-scale projects or projects which require higher quality \\nin terms of accuracy or reliability, testing may be performed by a third-party \\nIndependent Testing Contractor.  \\n \\n(b) \\nTest cases must be written for invalid and unexpected, as well as valid and expected \\ninput conditions.  A good test case is one that has a high probability of detecting \\nundiscovered errors.  A successful test case is one that detects an undiscovered error. \\n \\n(c) \\nA necessary part of a test case is defining the expected outputs or results. \\n \\n(d) \\nDo not plan testing effort on assumption that no errors will be found. \\n \\n(e) \\nIf much more errors are discovered in a particular section of a program than other \\nsections, it is advisable to spend additional testing efforts on this error-prone section \\nas further errors are likely to be discovered there.  \\n \\n(f) \\nTesting libraries should be set up allowing regression test to be performed at system \\nmaintenance and enhancement times. \\n \\n(g) \\nThe later in the development life cycle a fault is discovered, the higher the cost of \\ncorrection. \\n \\n(h) \\nSuccess of testing relies on complete and unambiguous specification.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"6.5\": {\n",
      "        \"title\": \"COMPLEMENTARY REVIEWS\",\n",
      "        \"content\": \"The testing mentioned in Section 6.3 essentially follow a Bottom-Up approach, which \\nhas one associated shortcoming, i.e. requirement and design errors could only be \\nidentified at a late stage.  To circumvent this, it is most important that the following \\nreviews should also be performed: \\n \\n(a) Requirements Review \\n \\n(i) to review the requirements specification with the objective to identify \\nrequirement items that are Incomplete, Incorrect, Inconsistent, or not testable. \\n(ii) proposed participants : e.g. business analysts, system analysts, Test Group, \\nusers and quality assurance staff (on user requirements specification). \\n \\n(b) Design Review \\n \\n(i) to review the design specification with the objective to identify design items \\nthat are Incomplete, Incorrect, Inconsistent, or not testable. \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    6-4 \\n  \\n \\n(ii) proposed participants : e.g. system analysts (on system design), Test Group, \\ncomputer operators (on operational procedures), business analysts (on \\nfunctional specifications), quality assurance staff, users (on user interface and \\nmanual procedures), domain architects (on architecture design) and where \\npossible domain experts from vendors (on specific domains such as storage and \\nnetwork infrastructure). \\n \\n(c) Program Walkthrough \\n \\n(i) to walkthrough, at least the most critical ones, program modules with the \\nobjective to identify errors as against the program specifications. \\n \\n(ii) proposed participants : e.g. system analysts, programmers, and lead \\nprogrammers. \\n \\n(Please refer to Section 11.2 regarding when these reviews should be carried out)\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"6.6\": {\n",
      "        \"title\": \"INDEPENDENT TESTING\",\n",
      "        \"content\": \"Due to the demands on improving quality of IT projects, third-party independent testing \\nis recommended to ensure that system functions are completed in good quality.  \\nIndependent testing may include various kinds of testing, e.g. unit test, functional test, \\nregression test, integration test, load test, accessibility test, and usability test, etc. \\n \\nThe independent Test Group conducts testing for the system with the aim to reveal defects \\nwhich cannot be found by the project team, which improves the quality of the system in \\nfulfilling project or business needs without any biases.  Independent Test Group \\nsometimes engages professional software testers to perform testing according to \\ninternationally recognised standards and methodologies which ensure the quality of \\ntesting in conformance to requirements of the project or business.\",\n",
      "        \"subsections\": {\n",
      "          \"6.6.1\": {\n",
      "            \"title\": \"Independent Testing Services by outsourced contractors\",\n",
      "            \"content\": \"The Independent Test Group may acquire external professional testing services from an \\nIndependent Testing Contractor to conduct various types of testing to help discover \\nhidden problems in the system.  The benefits of acquiring external independent testing \\nservices are:  \\n \\n(a) to improve the objectiveness and accuracy of test results, free from the influence of \\nthe developers or users; \\n(b) to meet industry or business standards, or comply with policies/regulations; \\n(c) to bring in expertise and skills which may not be available in the project team; \\n(d) to incorporate more effective and standardised quality control and failure analysis; \\n(e) to enhance the quality, availability and reliability of the software, reducing the costs \\nof remediation during maintenance; \\n(f) to release the project team to concentrate on development and implementation \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    6-5 \\n  \\nactivities; and \\n(g) to save costs in acquiring testing tools, facilities and equipment that may be used for \\none-off testing only. \\n \\nConsiderations for conducting independent testing by outsourced contractors:  \\n(a) System requirements are fixed and readily available. \\n(b) Business domain knowledge is relatively easy to learn and understand. \\n(c) Test process requires deeper/stronger/specialised skills which are not available in- \\nhouse. \\n(d) An independent third party\\u2019s certification or proof is required on the robustness of \\nthe system. \\n(e) Team members have less/no skills on testing. \\n(f) Project is developing a mission-critical system or a large-scale system. \\n(g) Project does not involve sensitive data or processes. \\n(h) It is a high-risk project. \\n \\nFor example, if a project develops mission-critical system in which the quality is very \\nimportant, it is helpful to employ an external Independent Testing Contractor to perform \\nsystem testing to assure the quality and reduce the number of defects.  If it is a high-risk \\nproject, it is necessary to acquire a professional testing contractor to help conduct function \\nor system testing to increase the project success rate. \\n \\nIf a project is a short-term, small and low-risk project, it may not be cost-effective to \\nacquire independent testing services.  Moreover, if the project team has already possessed \\nexperienced team members having skills on testing or utilising test-driven development \\napproach, there may not be a necessity to acquire independent testing services.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"6.6.2\": {\n",
      "            \"title\": \"Independent Testing by In-house Resources\",\n",
      "            \"content\": \"Independent testing may be conducted by an in-house team other than the project \\ndevelopment team; this can provide an impartial test result from the angle of an \\nindependent third-party, though not an external contractor. \\n \\nAs the in-house independent team has more business domain knowledge than that of the \\noutsourced contractors, time and resources required for understanding the business \\nrequirements before conducting testing will be saved. \\n \\nConsiderations for conducting independent testing by in-house resources:  \\n \\n(a) Business domain knowledge is difficult to learn and understand by outsourced \\ncontractors. \\n(b) Technical complexity of the project is high. \\n(c) Project involves sensitive data or processes. \\n(d) Team members have sufficient technical skills and experience in testing. \\n(e) There are undisclosed business know-how and processes. \\n(f) There are unique requirements on business domain knowledge. \\n(g) System requirements and test cases are volatile. \\n \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    6-2 \\n  \\nPlease refer to Appendix H for further details of independent testing services. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"7\": {\n",
      "    \"title\": \"LEVELS OF TESTING\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"7.1\": {\n",
      "        \"title\": \"UNIT TESTING\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"7.1.1\": {\n",
      "            \"title\": \"Scope of Testing\",\n",
      "            \"content\": \"Unit Testing (or Module Testing) is the process of testing the individual subprograms, \\nsubroutines, classes, or procedures in a program.  The goal here is to find discrepancy \\nbetween the programs and the program specifications prior to its integration with other \\nunits.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.1.2\": {\n",
      "            \"title\": \"Activities, Documentation and Parties Involved\",\n",
      "            \"content\": \"(a) Designers to include test guidelines (i.e. areas to be tested) in the design and program \\nspecifications. \\n(b) Programmers to define test cases during program development time. \\n(c) Programmers to perform Unit Testing after coding is completed. \\n(d) Programmers to include the final results of Unit Testing in the program \\ndocumentation.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.1.3\": {\n",
      "            \"title\": \"Practical Guidelines\",\n",
      "            \"content\": \"(a) Testing should first be done with correct data, then with flawed data. \\n(b) A program unit would only be considered as completed after the program \\ndocumentation (with testing results) of the program unit has been submitted to the \\nproject leader/SA. \\n(c) If there are critical sections of the program, design the testing sequence such that these \\nsections are tested as early as possible.  A \\u2018critical section\\u2019 might be a complex \\nmodule, a module with a new algorithm, or an I/O module. \\n(d) A unit testing would be considered as completed if the following criteria are satisfied: \\n(i) all test cases are properly tested without any errors; \\n(ii) there is no discrepancy found between the program unit and its specification; \\nand \\n(iii) program units of critical sections do not have any logical and functional errors. \\n \\n(e) Well-documented test cases can be reused for testing at later stages. \\n \\n(f) A team-testing approach may be applied in Unit Testing to improve the coverage of \\ntesting.  A team is formed consisting of two or more programmers.  The first \\nprogrammer will write the test case for the other programmer to code.  Upon \\ncompletion, the first programmer will conduct unit test on the program according to \\nthe test case.  This helps to improve the accuracy and reduce the defects found in the \\nprogram. \\n \\n(g) Please refer to Appendix A for a checklist on Unit Testing. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-2 \\n  \\n \\n(h) A code review may be performed after the Unit Testing.  A code review is a process \\nof inspecting the source code of a software program line-by-line carefully by one or \\nmore reviewers with the aid of automated testing tools, aiming to find out defects in \\nthe code before proceeding to the next stage of development.  It helps improve the \\nquality and maintainability of the program.\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"7.2\": {\n",
      "        \"title\": \"LINK/INTEGRATION TESTING\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"7.2.1\": {\n",
      "            \"title\": \"Scope of Testing\",\n",
      "            \"content\": \"Link/Integration Testing is the process of testing the linkages or interfaces between \\nprogram modules as against the system specifications.  The goal here is to find errors \\nassociated with interfacing.  As a by-product of the test process, the software modules \\nwould be integrated together. \\n \\nThis level of testing is often referred to as \\u201cIntegration Testing\\u201d, which is understood to \\nmean that the test process would end up with the software modules in integration i.e. \\nintegrated together.  Please note that the term \\u201cSystem Integration\\u201d means integration of \\nthe automated and manual operations of the whole system which is not the same as the \\nIntegration Test stated here.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.2.2\": {\n",
      "            \"title\": \"Activities, Documentation and Parties Involved\",\n",
      "            \"content\": \"(a) Test Group to prepare a Link/Integration Testing test plan. \\n(b) Test Group to prepare a Link/Integration Testing test specification before testing \\ncommences. \\n(c) Test Group, with the aid of the system analysts/programmers, to set up the testing \\nenvironment. \\n(d) Test Group to perform Link/Integration Testing; and upon fault found issue Test \\nIncident Reports to system analysts/programmers, who would fix up the liable errors. \\n(e) Test Group to report progress of Link/Integration Testing through periodic \\nsubmission of the Link/Integration Testing Progress Report. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-3\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.2.3\": {\n",
      "            \"title\": \"Practical Guidelines\",\n",
      "            \"content\": \"(a) Both control and data interface between the programs must be tested. \\n \\n(b) The following approaches can be applied during Link/Integration Testing: \\n \\n(i) \\nTop-down integration is an incremental approach to the assembly of software \\nstructure.  Modules are integrated by moving downward through the control \\nhierarchy, beginning with the main control module (\\u2018main program\\u2019).  \\nModules subordinate (and ultimately subordinate) to the main control module \\nare incorporated into the structure in either a depth-first or the breadth-first \\nmanner. \\n \\n(ii) \\nBottom-up integration, as its name implies, begins assembly and testing with \\nmodules at the lowest levels in the software structure.  Because modules are \\nintegrated from the bottom up, processing required for modules subordinate \\nto a given level is always available, and the need for stubs (i.e. dummy \\nmodules) is eliminated. \\n \\n(iii) \\nSandwich integration approach: it is a mix of the top-down and bottom-up \\napproaches.  It divides the software into three layers.  The bottom layer \\ncontains all the modules that are often invoked, and bottom-up approach is \\napplied to the integrated modules in this layer.  The top layer contains modules \\nimplementing major design decisions.  These modules are integrated by using \\nthe top-down approach.  The rest of the modules are put in the middle layer \\nwhich is integrated with the top layer and bottom layer, forming a complete \\napplication software.  This sandwich approach has the advantages of the top-\\ndown approach while writing dummy modules for the low-level module is not \\nrequired. \\n \\n(c) As a result of the testing, an integrated software should be produced. \\n \\n(d)  Please refer to Appendix B for a checklist on Link/Integration Testing. \\n \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-4\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"7.3\": {\n",
      "        \"title\": \"FUNCTION TESTING\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"7.3.1\": {\n",
      "            \"title\": \"Scope of Testing\",\n",
      "            \"content\": \"Function Testing is the process of testing the integrated software on a function by function \\nbasis as against the function specifications.  The goal here is to find discrepancy between \\nthe programs and the functional requirements.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.3.2\": {\n",
      "            \"title\": \"Activities, Documentation and Parties Involved\",\n",
      "            \"content\": \"(a) Test Group to prepare Function Testing test plan, to be endorsed by the PSC via the \\nPAT, before testing commences.  (Please refer Section 9.3) \\n(b) Test Group to prepare a Function Testing test specification before testing commences. \\n(c) Test Group, with the aid of the system analysts/programmers, to set up the testing \\nenvironment. \\n(d) Test Group (participated by user representatives) to perform Function Testing; and \\nupon fault found issue test incident reports to system analysts/programmers, who fix \\nup the liable errors. \\n(e) Test Group to report progress of Function Testing through periodic submission of the \\nFunction Testing Progress Report.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.3.3\": {\n",
      "            \"title\": \"Practical Guidelines\",\n",
      "            \"content\": \"(a) It is useful to involve some user representatives in this level of testing, in order to give \\nthem familiarity with the system prior to Acceptance test and to highlight differences \\nbetween users\\u2019 and developers\\u2019 interpretation of the specifications.  However, degree \\nof user involvement may differ from project to project, and even from department to \\ndepartment, all depending on the actual situation. \\n(b) User involvement, if applicable, could range from testing data preparation to staging \\nout of the Function Testing. \\n(c) It is useful to keep track of which functions have exhibited the greatest number of \\nerrors; this information is valuable because it tells us that these functions probably \\nstill contain some hidden, undetected errors. \\n(d) The following methods are useful in designing test case in Function Testing: \\n(i) \\nEquivalence Partitioning \\n \\nThis testing technique is to divide (i.e. to partition) a set of test conditions into \\ngroups or sets that can be considered the same.  It assumes that all conditions in \\none partition will be treated in the same way by the integrated software.  \\nTherefore, only one test case is required to be designed to cover each partition.  \\nFor example, an input field accepting integer values between -1,000 and +1,000 \\ncan be expected to be able to handle negative integers, zero and positive \\nintegers. The test data is partitioned into three equivalent acceptable set of \\nvalues.  In addition to numbers, this technique can also apply to any set of data \\nthat is considered as equivalent e.g. file type.  By using this technique, testing \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-5 \\n  \\none partition is equivalent to testing all of them. \\n \\n(ii) Boundary Value Analysis \\n \\nBoundary value testing is a technique to find whether the software will accept \\nthe expected range of values (e.g. maximum, minimum, just-inside and typical \\nvalues) and reject the out-of-range values (e.g. just-outside and error values), \\nfocusing on boundary/corner values of an input item/class.  This technique will \\nnormally design test cases for the boundary values, and a test case for a value \\nof acceptable range. \\n \\n(e) Please refer to Appendix C for a checklist on Function Testing.  \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-6\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"7.4\": {\n",
      "        \"title\": \"SYSTEM TESTING\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"7.4.1\": {\n",
      "            \"title\": \"Scope of Testing\",\n",
      "            \"content\": \"System Testing is the process of testing the integrated software with regard to the \\noperating environment of the system (e.g. Recovery, Security, Performance, Storage, etc.) \\n \\nIt may be worthwhile to note that the term has been used with different environments.  In \\nits widest definition especially for the small scale projects, it also covers the scope of \\nLink/Integration Testing and the Function Testing. \\n \\nFor small scale projects which combine the Link/Integration Testing, Function Testing \\nand System Testing in one test plan and one test specification, it is crucial that the test \\nspecification should include distinct sets of test cases for each of these three levels of \\ntesting.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.4.2\": {\n",
      "            \"title\": \"Activities, Documentation and Parties Involved\",\n",
      "            \"content\": \"(a) Test Group to prepare a System Testing test plan, to be endorsed by the PSC via the \\nPAT, before testing commences. \\n(b) Test Group to prepare a System Testing test specification before testing commences. \\n(c) Test Group, with the aid of the designers/programmers, to set up the testing \\nenvironment. \\n(d) Test Group (participated by the computer operators and user representatives) to \\nperform System Testing; and upon fault found issue test incident reports to the System \\nanalysts/programmers, who would fix up the liable errors. \\n(e) Test Group to report progress of the System Testing through periodic submission of \\nthe System Testing Progress Report.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.4.3\": {\n",
      "            \"title\": \"Practical Guidelines\",\n",
      "            \"content\": \"(a) 13 types of System Test are discussed below.  It is not claimed that all 13 types will \\nbe mandatory to every application system nor are they meant to be an exhaustive list.  \\nTo avoid possible overlooking, all of them should be explored when designing test \\ncases. \\n \\n(i) \\nVolume Testing \\n \\nVolume testing is to subject the system to heavy volumes of data under normal \\nworkload, and the attempt of which is to show that the system cannot handle the \\nvolume of data specified in its objective.  Since volume testing being obviously \\nexpensive, in terms of people time, one must not go overboard.  However every \\nsystem must be exposed to at least a few volume tests. \\n \\n(ii) Stress Testing \\n \\nStress testing involves subjecting the program to heavy loads or stress.  A heavy \\nstress is a peak volume of data encountered over a short span of time.  Although \\nsome stress test may experience \\u2018never will occur\\u2019 situations during its \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-7 \\n  \\noperational use, but this does not imply that these tests are not useful.  If errors \\nare detected by these \\u2018impossible\\u2019 conditions, the test is valuable, because it is \\nlikely that the same errors might also occur in realistic, less-stressful situations. \\n \\n(iii) Performance Testing \\n \\nMany programs have specific performance or efficiency objectives, such as \\nresponse times and throughput rates under certain workload and configuration \\nconditions.  Performance testing should attempt to show that the system does \\nnot satisfy its performance objectives. \\n \\n(iv) Recovery Testing \\n \\nIf processing must continue during periods in which the application system is \\nnot operational, then those recovery processing procedures/contingent actions \\nshould be tested during the System test.  In addition, the users of the system \\nshould be involved in a complete recovery test so that not only the application \\nsystem is tested but the procedures for performing the manual aspects of \\nrecovery are tested.  Examples on recovery testing are Disaster Recovery \\nTesting and Backup and Restore Testing. \\n \\n(v) \\nSecurity Testing \\n \\nSecurity testing is to test the adequacy of the security controls and procedures \\nimposed in the system by attempting to violate them.  Test cases are devised to \\nsubvert the imposed security checks and to search for security holes in existing \\nprograms.  For example, testing should attempt to access or modify data by an \\nindividual not authorised to access or modify that data.  Code review may be \\nconducted on programs to detect any insecure codes such as hard coding of \\npasswords. \\n \\n \\n(vi) Usability Testing \\n \\nUsability testing is to test the system in respect of its ease to understand, learn \\nand use; and its capability in achieving specified goals in a specified context of \\nuse.  The specified goals can be measured in relation to effectiveness, efficiency \\nand user satisfaction.  \\n \\n(vii) Procedure Testing \\n \\nComputer systems may not contain only computer processes but also involve \\nprocedures performed by people.  Any prescribed human procedures, such as \\nprocedures to be followed by the system operator, database administrator, or \\nterminal user, should be tested during the System test. \\n \\n(viii) Regression Testing \\n \\nRegression testing is the verification that what is being installed does not affect \\nany already installed portion of the application or other applications interfaced \\nby the new application. \\n \\n(ix) Operational Testing \\n \\nDuring the System test, testing should be conducted by the normal operations \\nstaff.  It is only through having normal operation personnel conduct the test that \\nthe completeness of operator instructions and the ease with which the system \\ncan be operated can be properly evaluated.  This testing is optional, and should \\nbe conducted only when the environment is available. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-8 \\n  \\n \\n(x) \\nScalability Testing \\n \\nScalability testing is to test the ability of the system to meet future efficiency \\nrequirements that may be beyond the current performance requirement. \\n \\n(xi) Installation Testing \\n \\nInstallation testing is to test the validity of written installation procedures for \\ninstalling the system onto the target environment i.e. to verify if the installation \\nprocedures are accurate and with understandable instructions. \\n \\n(xii) Compatibility (Co-existence) Testing \\n \\nIt focuses on testing the compatibility of the system with other co-existing \\nsoftware such as operating system or web browser which may affect the \\nbehaviour (e.g. resources usage) and stability of the system in the same \\nenvironment (e.g. on same hardware). \\n \\n(xiii) Adaptability Testing \\n \\nAdaptability testing is to test whether the system can function correctly in all \\nintended target environments with various specified components such as \\nhardware, software and operating system.  These environments are tested using \\na selection of functional test cases according to a predefined test procedure to \\nfind out any faults that may be introduced in adapting the software to a different \\nenvironment. \\n \\n \\n(b) It is understood that in real situations, due to possibly environmental reasons, some \\nof the tests (e.g. Procedures test, etc.) may not be carried out in this stage and are to \\nbe deferred to later stages.  Such deferral may be acceptable provided that the reasons \\nare documented clearly in the Test Summary Report and the test be carried out once \\nthe constraints removed. \\n \\n(c) Please refer to Appendix D for a checklist on System Testing. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-9\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"7.5\": {\n",
      "        \"title\": \"ACCEPTANCE TESTING\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"7.5.1\": {\n",
      "            \"title\": \"Scope of Testing\",\n",
      "            \"content\": \"Acceptance Testing is the process of comparing the application system to its initial \\nrequirements and the current needs of its end users.  The goal here is to determine whether \\nthe software end product is acceptable to its users and meets the business requirements.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.5.2\": {\n",
      "            \"title\": \"Activities, Documentation and Parties Involved\",\n",
      "            \"content\": \"(a) Test Group to prepare an Acceptance Testing test plan, which is to be endorsed by \\nthe PSC via the PAT. \\n \\n(b) Test Group to prepare an Acceptance Testing test specification, which is to be \\nendorsed by the PSC via the PAT. \\n \\n(c) Test Group, with the aid of the project team, to set up the testing environment; \\n \\n(d) Test Group to perform testing according to the Acceptance Test Plan; and upon fault \\nfound issue test incident reports to the system analysts/programmers, who will fix \\nup the liable error; \\n \\n(e) Test Group to report progress of the Acceptance Testing through periodic \\nsubmission of Acceptance Testing Progress Report; and \\n \\n(f) IPM to keep the overall Test Summary Report as documentation proof.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"7.5.3\": {\n",
      "            \"title\": \"Practical Guidelines\",\n",
      "            \"content\": \"(a) In general, Acceptance Testing conducted by the business users is often called \\u201cUser \\nAcceptance Testing\\u201d.  The objective is to ensure that system satisfies the acceptance \\ncriteria stated on the requirement document before releasing to daily operational \\nenvironment.  Sufficient effort and involvement of both the project team and end users \\nin User Acceptance Testing is of paramount importance in ensuring the quality of the \\nsystem.  The business impact and the cost and time for fixing a production problem \\nafter live run are higher than that during the acceptance stage.   Spending more effort \\nin User Acceptance Testing is not only a prudent approach in delivering quality \\nservices, but will also reduce the overall costs and impact at the maintenance stage. \\n \\n(b) Business users\\u2019 involvement is essential \\n \\nUser Acceptance Testing should be performed by business users to prove that a new \\nsystem works according to their understanding of their business requirements.  \\nBusiness users have the necessary knowledge and understanding of business \\nrequirements that IT testers may not have.  It is therefore essential to get the business \\nusers involved in the testing and not rely only on IT professionals.  Sufficient \\nresources on business users shall be planned and allocated to conduct the testing. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-10 \\n  \\n \\n(c) Effort of business users \\n \\nUser Acceptance Testing for projects, which have relatively clear requirements during \\ndevelopment stage, normally account for about 5% to 10% of the project \\nimplementation effort.  For those projects with complicated requirements that are not \\neasy to be specified clearly during development stage, the effort required for User \\nAcceptance Testing may be up to 20% of the project implementation effort.    \\n  \\nThe effort required by business users can be estimated based on the number and \\ncomplexity of test cases required to verify the system against the business \\nrequirements.  Detailed test cases may only be available at a later stage of the project, \\nnormally after System Analysis and Design.  Nonetheless, the effort can be estimated \\nbased on the following information when available: \\n \\n(i) the number of business processes and their level of complexity, which would \\ndetermine how much time is required to go through those processes and validate \\nthe acceptance criteria with the system;  \\n(ii) the number of functional requirements and their level of complexity (can be \\nmeasured by number of data sets required for a function), which would \\ndetermine how much time is required to go through those functions and validate \\nthe acceptance criteria with the system; \\n(iii) the number of inputs and outputs (screens, reports, interfaces, etc.) and their level \\nof complexity (can be measured by no. of fields on screens, reports, etc., and \\ntheir validation requirements), which would determine how much time is \\nrequired to go through those inputs and outputs and validate the acceptance \\ncriteria with the system; \\n(iv) the total number of test cases, the complexity and the time required for \\ncompleting these test cases based on the parameters mentioned above. \\n \\nThe number of rounds of the testing to be conducted shall also be considered for the \\ntotal effort required.  Two to three rounds of testing are normally considered the \\nminimum.  More rounds of testing shall be considered if the business requirements \\nare complicated or the quality of the development team is less assured.   \\n \\n(d) Following the User Acceptance Testing, an \\u201cOperational Acceptance Testing\\u201d is \\nconducted by computer operators to ensure that the system meets the operational \\nrequirement and provide confidence that the system will work properly before \\ndeployment. \\n \\n(e) For large-scale application systems, or systems involving new business or involving \\na large number of new users, it may be better if additional user testing is performed \\nprior to the User Acceptance Testing.  They are generally named as follows: \\n \\n (i) \\nAlpha Testing \\nThe testing is taken place at the development environment by some end users.  \\nDevelopers can observe problems while users are testing the system before \\ndeployment of the system to user testing environment for User Acceptance \\nTesting. \\n\\n \\nLEVELS OF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    7-11 \\n  \\n(ii) Beta Testing \\nThe testing is taken place at the user\\u2019s testing environment by users.  The \\nobjective of this testing is to determine the user satisfaction on the system and \\nthe fitness within the operational environment. \\n \\n(f) Precautions for IPM and project team \\n \\n(i) \\ncommunicate clearly to the users of their commitments on the testing \\n(ii) some users may be physically involved for the first time; therefore sufficient \\npresentation, introduction, and training will be very important \\n(iii) development team must be available to resolve problems if required \\n(iv) if possible, future maintenance team should be identified \\n(v) \\nensure all tasks are completed before handover to the maintenance team \\n \\n(g) Precaution for users \\n \\n(i) \\ntesting staff should be freed from their routine activities \\n(ii) commitment is authorised \\n \\n(h) Please refer to Appendix E for a checklist on User Acceptance Testing. \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"8\": {\n",
      "    \"title\": \"TEST DOCUMENTATION\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"8.1\": {\n",
      "        \"title\": \"INTRODUCTION\",\n",
      "        \"content\": \"The following summarises the test documentation (highlighted by the number in bracket) \\nto be produced in a project: \\n \\nFor each project \\n \\n \\nReferences of the five levels of testing as well as any \\nnecessary complementary reviews (refer to Section \\n6.5) in the project plan \\n(1) \\n \\n \\n \\n \\nFor each of the four levels of testing (i.e. \\nLink/Integration, Function, System and Acceptance \\nTesting) \\n \\n \\n \\n \\n \\nPrepare a test plan \\n(2) \\n \\n \\n \\n \\nPrepare a test specification \\n(3) \\n \\n \\n \\n \\nPrepare test incident reports for faults found (4) \\n \\n \\n \\n \\nPrepare periodic test progress reports \\n(5) \\n \\n \\n \\n \\nEnd Level \\n \\n \\n \\n \\n \\nPrepare test summary report after completion of the \\ntests \\n(6) \\n \\nEnd Project \\n \\nThe above documentation will be subject to quality assurance checks for existence and \\ncompleteness by the Quality Assurance Staff. \\n \\nNote: For small-sized projects, test plan and test specification as for Link/Integration \\nTesting, Function testing, System Testing could be combined. \\n \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-2\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"8.2\": {\n",
      "        \"title\": \"TEST PLAN\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"8.2.1\": {\n",
      "            \"title\": \"Purpose of Document\",\n",
      "            \"content\": \"To prescribe the scope, approach, resources, and schedule of testing activities for a level \\nof testing.  To identify the items being tested, the features to be tested, the testing tasks \\nto be performed, and the personnel responsible for each task. \\n \\nThis test plan should be prepared for each level of testing except Unit Testing.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.2.2\": {\n",
      "            \"title\": \"Outline of Document\",\n",
      "            \"content\": \"The test plan should provide at least the following information: \\n \\n(a) Test Items \\nList the functional items and software features to be tested. \\nFor Link/Integration Testing, list the software items to be tested (which in most of the \\ncases should be ALL software items). \\n \\nFor Function Testing, list the functions in the function catalogue (which in most cases \\nshould be ALL functions). \\n \\nFor System Testing, list the tests to be carried out. \\n \\n(b) Test Tasks \\n \\nNormally, there are the following 4 tasks: \\n1. \\nPrepare test specification, regarding \\n(i) \\nTest procedures \\n(ii) Test cases \\n(iii) Test data \\n(iv) Testing environment \\n2. \\nSet up of the testing environment \\n3. \\nLoad test data \\n4. \\nConduct tests \\n \\n(*Do not plan on the assumption that each test case will only be executed once) \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-3 \\n  \\nFor each task, list the estimated effort required and duration. \\n \\nFor example, \\n \\n \\nTask \\nNo. \\n \\nTask Description \\nEstimated \\nEffort \\n(man-\\nweeks) \\nRelative \\nCalendar week \\n0 1 2 3 4 5 6 7 8 9... \\n1 Prepare test specification on  \\n \\n \\n \\n-Test control procedures \\n# \\nXXXXX \\n \\n-Test cases \\n# \\nXXXX \\n \\n-Test data \\n# \\nXXX \\n \\n-Testing environment \\n# \\nXX \\n \\n \\n \\n \\n2 Set up of testing environment \\n# \\nX \\n \\n \\n \\n \\n3 Load test data \\n# \\nX \\n \\n \\n \\n \\n4 Conduct tests \\n# \\nXXXX \\n \\n \\n \\n \\n \\n(c) Responsibilities of relevant parties. For each party, specify their corresponding \\nresponsibilities for the testing levels. \\n \\n(d) Remarks. Describe any special constraint on the test procedures, identify any special \\ntechniques and tools requirements that are necessary for the execution of this test. \\n \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-4\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"8.3\": {\n",
      "        \"title\": \"TEST SPECIFICATION\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"8.3.1\": {\n",
      "            \"title\": \"Purpose of Document\",\n",
      "            \"content\": \"To specify refinements of the test approach, to identify the features to be tested, to specify \\nthe steps for executing the tests and specify the test case for each tests.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.3.2\": {\n",
      "            \"title\": \"Outline of Document\",\n",
      "            \"content\": \"The test specification should provide, to the least, the following information: \\n \\n(a) Test Control Procedures \\n \\nTo specify the following: \\n \\n(i) Error Reporting procedures; \\n(ii) Change / Version control procedures of S/W modules; and \\n(iii) Set-up and Wind-down procedures of the testing environment. \\n \\n(b) Testing Environment \\n \\nTo specify at least the following items that are required in the testing progress: \\n \\n(i) H/W and System S/W required; \\n(ii) Number of terminals/personal computers required; \\n(iii) Test facilities / tools required; \\n(iv) Test database; and \\n(v) Operations support / Operating hour. \\n \\n(c) Test Termination Criteria \\n \\nTo specify the criteria (e.g. Failing on certain critical test cases, when no. of error \\nreaches a certain limit, etc.) under which the testing would be terminated. \\n \\n(d) Test Cases \\n \\nIdentify and briefly describe the test cases selected for the testing.  For each test case, \\nstate its objective, specify also the steps and any special procedural requirements (e.g. \\nbring up screen, input data, keys pressed etc.), the  failure/pass criteria, the expected \\noutputs (e.g. message displayed, file changes, etc.), programs involved, inter-case \\ndependencies and specify whether the test cases has passed or failed after the testing \\nhas been conducted. \\n \\nIt should be noted that definition of test cases is a \\u201cdesign\\u201d process and do vary for \\ndifferent projects.  Please refer to Appendices A to F for test case design checklists. \\n \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-5\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"8.4\": {\n",
      "        \"title\": \"TEST INCIDENT REPORT\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"8.4.1\": {\n",
      "            \"title\": \"Purpose of Document\",\n",
      "            \"content\": \"To document any event that occurs during the test process which requires investigation.  \\nThe report is to be issued to the system analysts/programmers for the errors found in the \\ntesting progress.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.4.2\": {\n",
      "            \"title\": \"Outline of Document\",\n",
      "            \"content\": \"The test incident report should provide the following information: \\n \\n(a) Test-Incident-report Identifier \\n \\nSpecify the unique identifier assigned to the test incident report. \\n \\n(b) Summary \\n \\nSummarise the incident. \\n \\nIdentify the test items involved indicating their versions/revision levels. \\n \\n(c) Incident Description \\n \\nProvide a description of the incident.  This description should include the following \\nitems: \\n \\n(i) Inputs \\n(ii) Expected results \\n(iii) Anomalies \\n(iv) Date and time \\n(v) Procedure step \\n(vi) Environment \\n(vii) Testers and Observers \\n \\n(d) Impact \\n \\nIf known, indicate what impact this incident will have on test plan and test procedure \\nspecification. \\n \\n(e) Results of Investigation \\n \\n(i) Classification of Incident \\n1. \\nDesign / Program error \\n2. \\nError related to testing environment \\n3. \\nOthers \\n \\n(ii) Action taken \\n1. \\nDesign / Program changes. \\n2. \\nTesting Environment Changes \\n3. \\nNo action taken. \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-6 \\n  \\nSample Test Incident Report \\n \\nTest Incident Report  \\n \\n \\nTIR NO. : \\nDate : \\nReported By : \\n \\n \\n \\n \\n \\nIncident Description \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nImpact \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nResults of Investigation \\n \\nInvestigated By \\nDate \\n \\n \\nClassification \\nActions taken \\n \\n \\n() design/pgm error \\n() design/pgm changes \\n() error related to testing \\nenvironment \\n() testing environ\\u2019 \\nchanges \\n() others \\n() no action taken \\n \\n \\n \\n \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-7\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"8.5\": {\n",
      "        \"title\": \"TEST PROGRESS REPORT\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"8.5.1\": {\n",
      "            \"title\": \"Purpose of Document\",\n",
      "            \"content\": \"In order that progress of the test process is controlled properly, a periodic test progress \\nreport should be prepared by the Test Group, and submitted to the PAT / IPM. \\n \\nFrequency of the report is suggested to be weekly or bi-weekly.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.5.2\": {\n",
      "            \"title\": \"Terminology\",\n",
      "            \"content\": \"No. of test cases specified (#1): the total number of test cases that have been specified. \\n \\nNo. of test cases tried at least once (#2): the number of the specified cases that have been \\nput into test execution at least once.   \\n \\nThe percentage of #2 over #1 gives the percentage of the specified test cases that have \\nbeen executed at least once.  More importantly, the complement of this percentage gives \\nthe percentage of the specified test cases against which no test runs have ever been put so \\nfar. \\n \\nNo. of test cases completed: the number of the specified test cases that have been executed \\nand with the expected output generated. \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-8\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.5.3\": {\n",
      "            \"title\": \"Outline of Document\",\n",
      "            \"content\": \"(a) Sample Progress Control Summary \\n \\n \\nLevels of Testing : \\n \\n \\nReporting Period : \\n \\n \\n(In the reporting period) \\n(Over-all) \\n \\n \\n \\nNo. of tests cases specified \\n \\n(#1) \\n \\n \\n \\nNo. of test cases tried at least \\nonce \\n% \\n% \\n \\n(of #1) \\n(#2) \\n(of #1) \\n \\nNo. of test cases completed \\n% \\n% \\n \\n(of #1) \\n(of #1) \\n \\n \\n \\nNo. of Incident reports issued \\n \\n(#3) \\n \\n \\n \\nNo. of Incidents reports cleared \\n% \\n% \\n \\n(of #3) \\n(of #3) \\n \\n \\n \\nTest Group testing effort \\n(in man-hrs) \\n \\n \\n \\n \\n \\nDesigners / Programmers fault \\nclearing effort \\n(in man-hrs) \\n \\n \\n \\n \\n \\nUser testing effort \\n(in man-hrs) \\n \\n \\n \\nNotes: Testing effort should include ALL the effort directly related to the testing \\nactivities, but excluding the administration overhead. \\n \\n(b) Highlights of Outstanding Items and Reasons \\n \\nTo bring to the management attentions the problems encountered / foreseen and where \\npossible, the planned way of solving them. \\n \\n(c) Test Cases Results (Optional) \\n      Refer to Section 8.3.2 (d). \\n \\n\\n \\nTEST DOCUMENTATION \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    8-9\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"8.6\": {\n",
      "        \"title\": \"TEST SUMMARY REPORT\",\n",
      "        \"content\": \"\",\n",
      "        \"subsections\": {\n",
      "          \"8.6.1\": {\n",
      "            \"title\": \"Purpose of Document\",\n",
      "            \"content\": \"To summarise the results of the testing activities for documentation purpose and provide \\ninformation for future test planning references.\",\n",
      "            \"subsections\": {}\n",
      "          },\n",
      "          \"8.6.2\": {\n",
      "            \"title\": \"Outline of Document\",\n",
      "            \"content\": \"(a) Test cases Results \\n \\n \\n Refer to Section 8.3.2(d) \\n \\n(b) Remarks \\n \\n\\n \\nTEST PLANNING AND \\n \\nCONTROL \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "            \"subsections\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"9\": {\n",
      "    \"title\": \"TEST PLANNING AND CONTROL\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"9.1\": {\n",
      "        \"title\": \"TEST PLANNING\",\n",
      "        \"content\": \"As general practices, Link/Integration Testing, Function Testing, System Testing and \\nAcceptance Testing for the 3GL-based projects normally account to about 40% to 50% \\nof the project implementation effort.  (A higher percentage is expected for the 4GL-based \\nprojects). \\n \\nThe objective of test planning is to prepare the blueprint used by the project personnel \\nand users to ensure that the application system achieves the level of correctness and \\nreliability desired by the user. \\n \\nTest planning normally is accomplished by performing the following seven activities in \\nthe sequence indicated: \\n \\n1. \\nIdentification and inclusion of the testing activities in the project plan; \\n \\n2. \\nSetting up of the Test Group; \\n \\nFor each level, with the exception of Unit Testing, of testing, \\n \\n3. \\nPreparation of a test plan, which should define the scope of the testing and include \\ncriteria for ending testing; \\n \\n4. \\nDecide what test method, strategies, tools and resources to be used; \\n \\n5. \\nPreparation of test specification; \\n \\n6. \\nPurchase or develop test tools prior to the time when they will be needed; and \\n \\n7. \\nSet up of the testing environment.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"9.2\": {\n",
      "        \"title\": \"TEST CONTROL\",\n",
      "        \"content\": \"Test control is an activity that runs continuously during the test process to ensure that all \\nplanned tests are complete and test results are able to meet the test objectives, strategies \\nand mission.  Test control generally includes the comparison between the planned \\nprogress and the actual progress, application of appropriate corrective actions and \\nupdating of the test planning activities as necessary. \\n \\nSome test control actions that may be performed during the test process are: \\n(i) \\ndecision making based on information gathered and reported in test activities; \\n(ii) \\nresetting priority of tests when identified risk occurs e.g. late delivery of \\nprograms to be tested; and \\n(iii) \\nrescheduling of test activity because of late availability of the test environment.\\n\\n \\nAUTOMATED TOOLS FOR TESTING \\n \\n \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "        \"subsections\": {}\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"10\": {\n",
      "    \"title\": \"AUTOMATED TOOLS FOR TESTING\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"10.1\": {\n",
      "        \"title\": \"INTRODUCTION\",\n",
      "        \"content\": \"Because software testing often accounts for as much as 50% of all effort expended on a \\nsoftware implementation project, tools that can reduce test time (but without reducing \\nthoroughness) are very valuable.  For that purpose, use of the following types of \\nautomated tools would be most desirable.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"10.2\": {\n",
      "        \"title\": \"TEST DATA GENERATOR\",\n",
      "        \"content\": \"These are the tools to generate typical input data for programs that are undergoing testing.  \\nSystems Managers are advised to acquire one from vendors.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"10.3\": {\n",
      "        \"title\": \"STATIC ANALYSERS\",\n",
      "        \"content\": \"These are the tools to check about a program\\u2019s structure, complexity and maintainability.\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"10.4\": {\n",
      "        \"title\": \"DYNAMIC ANALYSERS\",\n",
      "        \"content\": \"It is also known as Test Coverage Verifiers.  These are the tools to measure internal test \\ncoverage of a program as to a given set of test data.   \\n \\n \\n\\n \\nSUMMARY \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "        \"subsections\": {}\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"11\": {\n",
      "    \"title\": \"SUMMARY\",\n",
      "    \"content\": \"\",\n",
      "    \"subsections\": {\n",
      "      \"11.1\": {\n",
      "        \"title\": \"TEST DOCUMENTATION AND PARTIES INVOLVED\",\n",
      "        \"content\": \"Type of \\nTesting \\nWhat is being \\ntested \\nTesting against \\n \\nTest data \\n \\nDone by \\nWho does sign-\\noff \\n \\nUnit Testing \\n \\nProgram units \\nsubprograms job \\ncontrol and \\nprocedures \\n \\nProgram \\nSpecification \\n \\nCorrect data \\nthen with \\nflawed data \\n \\nProgrammer \\n \\nLead \\nProgrammer \\n+ \\nSystem Analyst \\n \\nLink/ \\nIntegration \\nTesting \\n \\nLinkages/ \\ninterfaces between \\nprogram modules \\n \\n \\nProgram \\nSpecification \\n+ \\nSystem \\nSpecification \\n \\nControl and \\ndata  \\ninterface, \\nreturns/calls \\n \\n \\nTest Group \\n \\n \\nFunction \\nTesting \\n \\nIntegrated \\nsoftware on a \\nfunction by \\nfunction basis \\n \\nFunction \\nSpecification \\n \\n \\n \\nFunctions of \\nthe integrated \\nsoftware \\n \\n \\nTest Group \\n \\n \\nPSC \\nrecommended by \\nPAT \\n \\nSystem \\nTesting \\n \\nIntegrated \\nsoftware \\n \\nUser Objectives \\n+ \\nSystem \\nSpecification \\n \\n \\nUser supplied \\ntests data \\n \\n \\nTest Group \\n \\n \\nAcceptance \\nTesting \\n \\nEntire system \\nsimulated in \\nproduction \\nenvironment \\n \\n \\nUser \\nrequirements/ \\nacceptance \\ncriteria \\n \\nLive data \\nwhere possible \\n \\nUsers \\n \\n \\n\\n \\nSUMMARY \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n 11-2\",\n",
      "        \"subsections\": {}\n",
      "      },\n",
      "      \"11.2\": {\n",
      "        \"title\": \"TESTING ACTIVITIES IN SYSTEM DEVELOPMENT LIFE CYCLE\",\n",
      "        \"content\": \"System \\nAnalyst \\n \\nProgrammer \\nTest \\nGroup \\n \\nUser \\nComputer \\nOperator \\nPAT \\nFeasibility \\nStudy \\n \\n \\n \\n \\n \\n \\nSystem \\nAnalysis \\n-  Perform \\nRequirement \\nReview \\n \\n- Prepare Project \\nPlan \\n \\n \\n \\nSystem \\nDesign \\n-  Include test \\nguidelines in \\nDesign and \\nProgram \\nSpecification \\n-  Perform \\nDesign Review \\n \\n- Prepare \\nLink/Integration \\nTest Plan  \\n- Prepare Function \\nTest Plan \\n- Prepare System \\nTest Plan \\n- Prepare Test \\nSpecification \\n- Prepare \\nAcceptance \\nTest Plan \\n (Project team \\nto assist user \\nto prepare the \\nAcceptance \\nTest Plan) \\n \\n- Recommend \\nendorsement of \\nthe  Link/ \\nIntegration, \\nFunction, System \\nand Acceptance \\nTest Plan  \\n- Recommend \\nendorsement of  \\nProgram \\nDevelopment \\n \\n-  Perform Program \\nWalkthrough \\n- Include extra test \\ncases \\n- Perform Unit \\nTesting \\nfor each of the \\nLink/Integration, \\nFunction and \\nSystem Testing \\n- Set up testing \\nenvironment \\n- Prepare \\nAcceptance \\nTest \\nSpecification \\n \\nthe Test \\nSpecification \\nSystem \\nIntegration \\nand Tests \\n \\n \\n- Perform \\nLink/Integration \\nTesting \\n- Perform Function \\nTesting \\n- Perform System \\nTesting \\n- Perform \\nAcceptance \\nTesting rehearsal \\n \\n- Participate \\nin System \\nTest on \\noperation \\nprocedure \\n- Accept \\noperation \\nprocedure \\n- Ensure smooth \\nrunning of testing \\nactivities \\n- Recommend \\nendorsement of \\ntest results \\nUser \\nAcceptance \\n \\n \\n \\n- Perform \\nAcceptance \\nTest  \\n- Accept the \\nsystem\",\n",
      "        \"subsections\": {}\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_1.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_2.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_3.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_4.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_5.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_6.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_7.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/Json Files/g20_pub_json_8.json' already exists. Saving with different name.\n",
      "Attempting to save JSON to: gs://genai_ex_documents/Json Files/g20_pub_json_9.json\n",
      "Successfully saved JSON to GCS at: gs://genai_ex_documents/Json Files/g20_pub_json_9.json\n",
      "Reading JSON from: gs://genai_ex_documents/Json Files/g20_pub_json_9.json\n",
      "{\n",
      "  \"title\": \"OVERVIEW\",\n",
      "  \"content\": \"This document, in essence, suggests a reference model for the planning and conduct of \\nApplication Software Testing.  The following serves as an overview of the model:\",\n",
      "  \"subsections\": {\n",
      "    \"5.1\": {\n",
      "      \"title\": \"PROJECT ORGANISATION\",\n",
      "      \"content\": \"\",\n",
      "      \"subsections\": {\n",
      "        \"5.1.1\": {\n",
      "          \"title\": \"Project Organisation Structure\",\n",
      "          \"content\": \"A project organisation is set up to champion, manage and execute an IT project.  Members \\nof the project organisation include the Project Owner, Project Steering Committee (PSC), \\nProject Assurance Team (PAT), Internal Project Manager (IPM) and Business Analyst \\n(BA). \\n \\nThe PSC champions the project and is the ultimate decision-maker for the project.  It \\nprovides steer and support for the IPM and endorses acceptance of project deliverables. \\nThe IPM manages the project and monitors the project implementation on a day-to-day \\nbasis for the Project Owner/the PSC. \\n \\nThe PAT is responsible for overseeing project progress and managing quality assurance \\nactivities, which include: \\n(a) recommending the test plans, test specifications and test summary report for \\nendorsement by the PSC; and \\n(b) co-ordinating, monitoring and resolving priority conflicts on the testing activities to \\nensure smooth running of testing activities.  \\n \\nPlease refer to the Practice Guide to Project Management for IT Projects under an \\nOutsourced Environment (PGPM) for more details of the project organisation.\",\n",
      "          \"subsections\": {}\n",
      "        },\n",
      "        \"5.1.2\": {\n",
      "          \"title\": \"Test Group\",\n",
      "          \"content\": \"Testing is the process of executing a program with the intent of finding errors.  Since it is \\nsuch a \\u201cdestructive\\u201d process, it may be more effective and successful if the testing is \\nperformed by an independent third party other than the original system analysts / \\nprogrammers. \\n \\nAs far as possible, testing should be performed by a group of people different from those \\nperforming design and coding of the same system.  That group of people is called the Test \\nGroup. \\n \\nA Test Group can be set up to carry out the testing activities especially for large-scale \\nprojects or projects involving a large number of users.  The emphasis here is on the \\nindependent role of the Test Group, which does not necessarily mean dedicated resources.  \\nThe necessity of an independent Test Group should be determined at the project \\ninitialisation stage through an assessment based on project complexity, criticality, \\nimplementation schedule and other risks.  The type(s) of testing to be performed \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-2 \\n  \\nindependently and the high level estimation of additional resources, if required, should be \\ndetermined and planned for respectively as early as possible. \\n \\nThe following figure shows an example of project organisation with the formation of a \\nTest Group and an optional Independent Testing Contractor providing independent testing \\nservice.   It is noted that the independent testing may be conducted by a Test Group of in-\\nhouse staff members as well as by external contractor.  \\n \\n \\n \\nFigure 1 - Example of Project Organisation with Test Group and Independent \\nTesting Contractor\",\n",
      "          \"subsections\": {}\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"5.2\": {\n",
      "      \"title\": \"TESTING ACTIVITIES\",\n",
      "      \"content\": \"With reference to the Agile Software Development Method, test preparation should be \\nstarted as early as possible and constant communication should be maintained with \\nrelevant stakeholders to facilitate collaboration and transparency.  The following activities \\nare suggested: \\n \\n(i) \\nThe IPM should develop a high level test plan covering all major test types during \\nproject initiation with the objectives, scope of testing and the composition of Test \\nGroup including contractors, business users and internal IT staff with defined \\nroles and responsibilities. \\n \\n(ii) \\nContractor project manager (or IPM for in-house developed project) to enrich the \\ntest plans by engaging his/her staff to draft test cases; internal IT staff to check all \\nmajor test plans; and business users to provide different test cases to address \\ndifferent scenarios; and \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-3 \\n  \\n \\n(iii) Contractor project manager (or IPM for in-house developed project) to maintain \\nongoing communication and collaboration among stakeholders by distributing all \\nmajor test plans and feedbacks to stakeholders regularly to keep them informed \\nthe project progress throughout the whole system development stage. \\n \\nA computer system is subject to testing from the following five different perspectives: \\n \\n(i) \\nTo validate individual program modules against program specifications (Unit \\nTesting); \\n \\n(ii) \\nTo validate linkages or interfaces between program modules against design \\nspecifications (Link/Integration Testing); \\n \\n(iii) To validate integrated software against functional specifications (Function \\nTesting); \\n \\n(iv) To validate the integrated software against specifications on operating \\nenvironment (System Testing); and, \\n \\n(v) \\nTo validate the integrated software against end-user needs and business \\nrequirements (Acceptance Testing). \\n \\n(Refer to Section 7) \\n\\n \\nOVERVIEW \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________ \\n \\n    5-4\",\n",
      "      \"subsections\": {}\n",
      "    },\n",
      "    \"5.3\": {\n",
      "      \"title\": \"TEST DOCUMENTATION\",\n",
      "      \"content\": \"To document testing activities through the use of \\n \\n(i) \\nTest Plan \\n(ii) \\nTest Specification \\n(iii) \\nTest Incident Report \\n(iv) \\nTest Progress Report \\n(v) \\nTest Summary Report \\n \\n(Refer to Section 8)\",\n",
      "      \"subsections\": {}\n",
      "    },\n",
      "    \"5.4\": {\n",
      "      \"title\": \"TEST PLANNING AND CONTROL\",\n",
      "      \"content\": \"\",\n",
      "      \"subsections\": {\n",
      "        \"5.4.1\": {\n",
      "          \"title\": \"Progress Control\",\n",
      "          \"content\": \"Monitor the day-to-day progress of the testing activities through the use of Test Progress \\nReports. \\n \\n(Refer to Section 8.5)\",\n",
      "          \"subsections\": {}\n",
      "        },\n",
      "        \"5.4.2\": {\n",
      "          \"title\": \"Quality Control / Assurance\",\n",
      "          \"content\": \"Testing documentation to be compiled by Test Group or Independent Testing Contractor \\nif outsourced, cross-checked by quality assurance staff2, and reviewed by the PAT.\",\n",
      "          \"subsections\": {}\n",
      "        },\n",
      "        \"5.4.3\": {\n",
      "          \"title\": \"Resource Estimation\",\n",
      "          \"content\": \"Project teams may update the testing metrics information to a centralised database for \\nfuture test planning references. \\n \\n                                                 \\n2 Quality assurance staff should be the IPM or other delegated staff.  However, those who are the members of the Test \\nGroup should not take up the quality assurance role for the project if the tests are conducted by them but not by \\nIndependent Testing Contractor. \\n\\n \\nGENERAL CONCEPTS \\n \\nOF TESTING \\n________________________________________________________________________________ \\n \\n_______________________________________________________________________________\",\n",
      "          \"subsections\": {}\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_1.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_2.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_3.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_4.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_5.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_6.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_7.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_8.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_9.json' already exists. Saving with different name.\n",
      "Attempting to save JSON to: gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_10.json\n",
      "Successfully saved JSON to GCS at: gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_10.json\n",
      "Reading JSON from: gs://genai_ex_documents/Json Files/g20_pub_json_9.json\n",
      "Found Heading Number: 5.4.1\n",
      "Section Data:\n",
      "{\n",
      "  \"title\": \"Progress Control\",\n",
      "  \"content\": \"Monitor the day-to-day progress of the testing activities through the use of Test Progress \\nReports. \\n \\n(Refer to Section 8.5)\",\n",
      "  \"subsections\": {}\n",
      "}\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_1.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_2.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_3.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_4.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_5.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_6.json' already exists. Saving with different name.\n",
      "File 'gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_7.json' already exists. Saving with different name.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x00000154D1748C20>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to save JSON to: gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_8.json\n",
      "Successfully saved JSON to GCS at: gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x00000154D1C360D0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000154D15A00B0>, 571375.8840818)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x00000154D1C35D10>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Tool Call] print_locations triggered by retrieval_agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< Agent Response: {\"Location_1\" : \"gs://genai_ex_documents/retreived_content_files/g20_pub_5_json_10.json\", \"Location_2\" : \"gs://genai_ex_documents/retreived_content_files/g20_pub_Progress Control_json_8.json\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x00000154D1BBA0D0>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000154D15A00B0>, 571391.5706028)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x00000154D1BBA210>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\n",
    "        \"GCS File Location : gs://genai_ex_documents/main_docs/g20_pub.pdf , User Prompt:Give me the test cases corresponding to section 5 and 'Progress Control'\",\n",
    "        runner=runner,\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID\n",
    "    )\n",
    "\n",
    "await run_conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65045bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
